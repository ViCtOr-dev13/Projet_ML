#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon May  1 20:08:44 2023

@author: yannt
"""

# Import
#------------------------------------------------------------------------------
import warnings
import pandas as pd
import numpy as np
import json

import src.constant as C
import src.function.function as f


from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier


#------------------------------------------------------------------------------
warnings.filterwarnings('ignore')
#------------------------------------------------------------------------------

target = C.TARGET
target_best = C.TARGET_BEST

df_all = pd.read_csv(C.PATH_DATASET + C.ALL)
rf_all = RandomForestClassifier()
f.replace_nan(df_all)
f.drop_na_target(df_all, target)
f.replace_target(df_all, target, C.REPLACE_ALL, C.REPLACE_1)
f.fill_nan_mean(df_all)


df_all_best = pd.read_csv(C.PATH_DATASET + C.BEST_ALL)
rf_all_best = RandomForestClassifier()
f.replace_nan(df_all_best)
f.drop_na_target(df_all_best, target_best)
f.replace_target(df_all_best, target_best, C.REPLACE_ALL, C.REPLACE_1)
f.fill_nan_mean(df_all_best)



(X_train, X_test,X_validate, 
 y_train, y_test, y_validate) = f.split_dataframe(df_all, 
                                                   target)
                                                  
(X_train_best, X_test_best, X_validate_best, 
 y_train_best, y_test_best, y_validate_best) = f.split_dataframe(df_all_best, 
                                                   target_best)                                                  


# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 200)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 200, num = 20)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}


rf_random = RandomizedSearchCV(estimator = rf_all,
                               param_distributions = random_grid, 
                               n_iter = 50 , cv = 3, verbose = 3,
                               random_state=42, n_jobs = -1)

    

rf_random.fit(X_train, y_train)


rf_random_best = RandomizedSearchCV(estimator = rf_all_best,
                               param_distributions = random_grid, 
                               n_iter = 50 , cv = 3, verbose = 3, 
                               random_state=42, n_jobs = -1)

    

rf_random_best.fit(X_train_best, y_train_best)

    
output = {}
output['best_params'] = rf_random.best_params_
output['best_estimator'] = str(rf_random.best_estimator_)
output['best_score'] = rf_random.best_score_

output['best_params_best'] = rf_random_best.best_params_
output['best_estimator_best'] = str(rf_random_best.best_estimator_)
output['best_score_best'] = rf_random_best.best_score_


with open('../../result/randomizedsearch_all.json', 'w', encoding='utf8') as outfile:
    json.dump(output, outfile, indent = 4, ensure_ascii=False)
